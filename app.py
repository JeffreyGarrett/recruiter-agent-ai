import os
os.umask(0)
import sys
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from dotenv import load_dotenv
import streamlit as st
# custom ageent imports
from agent.fit_evaluator import evaluate_candidate_fit
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings
from llama_index.core.node_parser import SentenceSplitter
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.llms.openai import OpenAI
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.core.storage import StorageContext
import chromadb
import shutil


# Load environment variables
load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")

# Print all environment variables loaded from .env
# print("Loaded environment variables:")
# print(f"OPENAI_API_KEY = {os.getenv('OPENAI_API_KEY')}")

# Configure LLM and embedding models globally
Settings.llm = OpenAI(model="gpt-4o", api_key=openai_api_key)
Settings.embed_model = OpenAIEmbedding(api_key=openai_api_key)

# Set up ChromaDB vector store

persist_dir = "./vector_store"
if os.path.exists(persist_dir):
    shutil.rmtree(persist_dir)
    print("üßπ Cleared old vector store.")   

chroma_client = chromadb.Client()
chroma_collection = chroma_client.get_or_create_collection("jeff_docs")

vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
storage_context = StorageContext.from_defaults(vector_store=vector_store)


# Load documents
documents = SimpleDirectoryReader("docs").load_data()
print(f"üìÑ Loaded {len(documents)} documents.")

# Use SentenceSplitter to create real chunks
parser = SentenceSplitter(chunk_size=512, chunk_overlap=50)
nodes = parser.get_nodes_from_documents(documents)
print(f"üîñ Generated {len(nodes)} chunks.")

# Build index from pre-chunked nodes
index = VectorStoreIndex(nodes, storage_context=storage_context)
index.storage_context.persist(persist_dir)
os.chmod("./vector_store", 0o777)

print("üìä Chroma vector count:", chroma_collection.count())

# Create query engine
query_engine = index.as_query_engine(similarity_top_k=5, verbose=True, llm=OpenAI(
        model="gpt-3.5-turbo",
        api_key=openai_api_key,
        system_prompt="You are JeffBot, a friendly and insightful Director of Technical Product and engineering executive. Respond as if you are Jeff with a profressional leadership and friendly tone and with a quick pun everyonce in a while. Whenever possible you want to provide details and exampels to drive confidence in your ability to lead and deliver results. Try to make people smile when you respond",
        text_qa_template="""
            You are JeffBot, an AI that answers questions based on Jeff Garrett's resume and professional background. 
            Always respond in a tone that is confident, insightful, and grounded, but avoid sounding overly formal.

            When answering, focus on:
            - Jeff‚Äôs leadership experience
            - His passion for public service and tech modernization
            - Real-world, practical impact

            If the question is outside the scope of the data, politely let the user know.

            Context: {context_str}
            Question: {query_str}
            Answer:
            """
    ))

# Streamlit UI
st.set_page_config(page_title="Chat with Jeff", page_icon="ü§ñ")
st.title("ü§ñ Ask Jeff Anything")
st.markdown("This AI has read my professional history. Ask away!")

st.subheader("üìÑ Job Description & Evaluation Criteria")

job_description = st.text_area(
    "Paste the job description here",
    placeholder="We're hiring a VP of Engineering to lead a multi-disciplinary team and drive platform strategy..."
)

must_haves = st.text_area(
    "What are the must-have qualifications or skills?",
    placeholder="e.g., 10+ years engineering leadership, experience in platform/infra, agile delivery at scale..."
)

nice_to_haves = st.text_area(
    "What are the nice-to-haves?",
    placeholder="e.g., Public sector experience, ML/AI familiarity, distributed org management..."
)

import re

with open("docs/resume.md", "r") as f:
    resume_text = f.read()


if st.button("üîç Evaluate Jeff‚Äôs Fit for This Role"):
    if job_description.strip() and must_haves.strip():
        with st.spinner("Evaluating..."):
            result = evaluate_candidate_fit(
                resume_text=resume_text,
                job_description=job_description,
                must_haves=must_haves,
                nice_to_haves=nice_to_haves,
                api_key=openai_api_key
            )

            # DEBUG: always show full raw result first
            # st.markdown("### üìÑ Full Evaluation Output")
            # st.code(result, language="markdown")

            # More forgiving parsing (ignores **, :, etc.)
            decision_match = re.search(r"(?i)\**\s*decision\**.*?:\s*(.+)", result)
            confidence_match = re.search(r"(?i)\**\s*confidence.*?\**.*?:\s*(\d+)", result)
            strengths_match = re.search(r"(?i)\**\s*strengths.*?\**.*?:\s*(.+?)(?=\n\s*(?:\**\s*gaps|\Z))", result, re.DOTALL)
            gaps_match = re.search(r"(?i)\**\s*gaps.*?\**.*?:\s*(.+)", result, re.DOTALL)

            st.markdown("### ü§ñ Parsed Summary")

            if decision_match:
                st.subheader(f"üü¢ Decision: {decision_match.group(1).strip()}")

            if confidence_match:
                st.markdown(f"**Confidence Score:** {confidence_match.group(1)} / 100")

            if strengths_match:
                st.markdown("**‚úÖ Strengths / Alignments:**")
                st.markdown(strengths_match.group(1).strip())

            if gaps_match:
                st.markdown("**‚ö†Ô∏è Gaps or Unknowns:**")
                st.markdown(gaps_match.group(1).strip())
    else:
        st.warning("Please provide at least the job description and must-have criteria.")




user_input = st.text_input("Your question:")


if user_input:
    with st.spinner("Thinking..."):
        response = query_engine.query(user_input)
        st.markdown(f"**Answer:** {response.response}")